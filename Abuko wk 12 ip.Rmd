---
title: "Moringa R EDA IP"
author: "LAURA ABUKO"
date: "2022-07-18"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## INTRODUCTION

# **Research Question**
 To identify which individuals are most likely to click on her ads. 
 
## **1.Defining the Question**

### **1.1 Specifying our Objective**
Our main aim is to do thorough exploratory data analysis for univariate and bivariate data and come up with recommendations for our client.

### **1.2 Metric For Success**
Building an  elaborate visualizations for univariate and bivariate analysis

### **1.3 Recording Experimental design**
- Loading the data
-Checking the data
- Tidying the data
- Univariate Analysis
- Bivariate Analysis
- Challenging the solution
- Recommendations
- Follow up questions

### **1.4 Relevance of Data**
Data used was relevant for this analysis

## ***2.Loading Data***

```{r}

df <-read.csv("//home//lau/Downloads//lau.csv", header=TRUE,sep =",")

View(df)

```

## ***Checking the data***
### ***viewing the top 6 entries***
```{r}
head(df)
```

### ***Viewing the bottom 6 entries***
```{r}
tail(df)
```

### ***Viewing the data types of our columns***
```{r}
lapply(df,class)
```
 
## ***3. Tidying the data***
### ***Checking for null values***
```{r}
colSums(is.na(df))
```
### ***Checking for duplicates***
```{r}
sum(duplicated(df))
```
```{r}
sum(!duplicated(df))
```
There are no duplicates in our data

### ***checking for outliers***
```{r}
boxplot(df$`Area.Income`,main="Boxplot for Area.Income",col = "purple")
```
```{r}
boxplot(df$`Daily.Internet.Usage`,main="Boxplot for Daily.Internet.Usage",col="brown")
```

```{r}
boxplot(df$`Daily.Time.Spent.on.Site`,main="Boxplot for Male",col = "red")
```

```{r}
boxplot(df$`Age`,main="Boxplot for Age",col = "orange")
```


Area.income was the only column that had outliers

#we will nor remove outliers as it may be due to variability in our data


## ***4. Univariate Analysis***
### ***4.1. Measures of central Tendency***
#### ***4.1.1. Mean***
```{r}
print("The mean for numeric variables is:")
colMeans(df[sapply(df,is.numeric)])
```
#### ***4.1.2. Median***
```{r}
cat("The median for daily time spent on site is",median(df$Daily.Time.Spent.on.Site))
cat("\n")
cat("The median for age is",median(df$Age))
cat("\n")
cat("The median for Area.Income is",median(df$Area.Income))
cat("\n")
cat("The median for daily Internet Usage is",median(df$Daily.Internet.Usage))
cat("\n")


cat("The median for Clicked on Ad",median(df$Clicked.on.Ad))
```
#### ***4.1.3. Mode***




#### ***4.1.4. Standard Deviation***
```{r}
cat("The standard deviation for daily time spent on site is",sd(df$`Daily Time Spent on Site`))
cat("\n")
cat("The standard deviation for age is",sd(df$`Age`))
cat("\n")
cat("The standard deviation for Area.Income is",sd(df$`Area Income`))
cat("\n")
cat("The standard deviation for daily Internet Usage is",sd(df$`Daily Internet Usage`))
cat("\n")

cat("The standard deviation for Clicked on Ad",sd(df$`Clicked on Ad`))
```
#### ***4.1.5. Variance***

```{r}

cat("The variance for age is",var(df$`Age`))
cat("\n")

```
seeing as the age variable was the only one with a standard deviation its only logical for it alone to have variance


### ***4.2. Measures of Dispersion***
#### ***4.2.1. Minimum***
```{r}
install.packages("dplyr")
library(dplyr)
### ***4.2. Measures of Dispersion***
#### ***4.2.1. Minimum***
```{r}
library(dplyr)
df %>% summarise_if(is.numeric,min)
```
#### ***4.2.2. Maximum***
```{r}
#Maximum of the columns
df %>% summarise_if(is.numeric,max)
```


#### ***4.2.2. Maximum***
```{r}
#Maximum of the columns
df %>% summarise_if(is.numeric,max)
```
#### ***4.2.3. Range***
```{r}
cat("The range for daily time spent on site is",range(df$"Daily.Time.Spent.on.Site"))
cat("\n")
cat("The range for age is",range(df$"Age"))
cat("\n")
cat("The range for Area.Income is",range(df$"Area.Income"))
cat("\n")
cat("The range for daily Internet Usage is",range(df$"Daily.Internet.Usage"))
cat("\n")



```
#### ***4.2.4. Quantile***
```{r}
cat("The Quantile for daily time spent on site is",quantile(df$"Daily.Time.Spent.on.Site"))
cat("\n")
cat("The Quantile for age is",quantile(df$"Age"))
cat("\n")
cat("The Quantile for Area.Income is",quantile(df$"Area.Income"))
cat("\n")
cat("The Quantile for daily Internet Usage is",quantile(df$"Daily.Internet.Usage"))
cat("\n")

cat("The Quantile for Clicked on Ad",quantile(df$"Clicked.on.Ad"))
```
####Summary
```{r}
summary(df)
```
#### ***Barcharts***
```{r}
frequency <- table(df$Male)
frequency
barplot(frequency,col=c("Cyan","lightgreen"),main="Barchart for Male",xlab = "Male",ylab = "Total Count")
```
```{r}
frequency1 <- table(df$"Clicked.on.Ad")
frequency1
barplot(frequency1,col=c("Brown","yellow"),main="Barchart for Clicked on Ad",xlab = "Clicked on Add",ylab = "Total Count")
```
```{r}
frequency2 <- table(df$Age)
frequency2
barplot(frequency2,main="Barchart for Age",xlab = "Age",ylab = "Total Count")
```
#### ***Histograms***
```{r}
install.packages("ggplot2")
library(ggplot2)
ggplot(df, aes(Age)) + geom_histogram(bins = 20, color = 'blue') + 
    labs(title = 'Age distribution', x = 'Age', y = 'Frequency')
```
###Bivariate Analysis
####Covariance
```{r}
covariance <- cov(df[,sapply(df,is.numeric)])
covariance
```
####Correlation
```{r}
install.packages("corrplot")
library(corrplot)
```
```{r}
#Correlation of all numeric variables
df.cor = cor(df[,sapply(df,is.numeric)],method = c("spearman"))
df.cor
```
####correlation plot
```{r}
corrplot(df.cor,method="number",main="Correlatio of numerical variables")
```
####Heat Map
```{r}
heatmap(x=df.cor,symm = TRUE,method="number")
```

time spent on the site columns have a large positive correlation and so does the Clicked.On.Ad and age columns.


####Scatter plots
```{r}
#Scatter plot for area in income vs daily internet usage
ggplot(df, aes(x = "Area.Income", y = "Daily.Internet.Usage" )) +
  geom_point() + labs(title = 'Scatter plot for area in income vs daily internet usage')
```

```{r}
#Scatter plot for age vs daily time spent on site
ggplot(df, aes(x = "Age", y = "Daily.Time.Spent.on.Site" )) +
  geom_point() + labs(title = 'Scatter plot for age vs daily time spent on site')
```

```{r}
#Scatter plot for age vs area in income
ggplot(df, aes(x = "Age", y = "Area.Income", col=Male),add ="reg.line",conf.int = TRUE) +geom_point()+geom_smooth()+ labs(title = 'Scatter plot age vs area in income')
```


```{r}
library(dplyr)
```
**Daily internet usage per country**
```{r}
df %>% group_by(Country, "Daily.Internet.Usage")%>% head(10)%>% arrange(desc("Daily.Internet.Usage"))
```
**Daily Time Spent on Site per Country**
```{r}
df %>% group_by("Country", "Daily.Time.Spent.on.Site")%>% head(10)%>% arrange(desc("Daily.Time.Spent.on.Site"))
```



## ***Conclusions***

* Ages between  26 and 42 record the highest frequency of ad clicks on the site and also the highest amount of time spent on the internet. 

*Income levels between 50k to 70k record the highest frequency of ad clicks on the site. People who spend more time on the internet have a high income. 

## ***Reccommendations***

The ads posted on the client’s site should be more relevant to this demographic between 26-42 years of age. Her users also skew more on the high income end of the spectrum. This was expected considering her age demographic data.

The client should consider curating specific advertising content for the top 10 countries spending more time on the internet so that they can spend some amount of time on her site.


```{r}
install.packages("tinytex")
```
## ** Modelling**
```{r}
# REQUIREMENT:  create a supervised learning model to help identify which individuals are most likely to click on the ads in the blog.
## Packages Needed
install.packages("tidyverse")

```

##### **Label encoding**
```{r}
#install.packages("superml",dependencies = TRUE, repos = 'http://cran.rstudio.com/')
install.packages("superml")
```

```{r}
library(superml)
print("Data before label encoding..\n")
print(df)
label <- LabelEncoder$new()
#Label encoding
df$City <- label$fit_transform(df$City)
print(df$City)
df$Country <- label$fit_transform(df$Country)
print(df$Country)
df$Ad.Topic.Line <- label$fit_transform(df$Ad.Topic.Line)
print(df$Ad.Topic.Line)## ** Modelling**
```{r}
# REQUIREMENT:  create a supervised learning model to help identify which individuals are most likely to click on the ads in the blog.
## Packages Needed
library(tidyverse)
```

##### **Label encoding**
```{r}
#install.packages("superml",dependencies = TRUE, repos = 'http://cran.rstudio.com/')
```

```{r}
library(superml)
print("Data before label encoding..\n")
print(df)
label <- LabelEncoder$new()
#Label encoding
df$City <- label$fit_transform(df$City)
print(df$City)
df$Country <- label$fit_transform(df$Country)
print(df$Country)
df$Ad.Topic.Line <- label$fit_transform(df$Ad.Topic.Line)
print(df$Ad.Topic.Line)


### **Supervised Learning**
##### KNN
```{r}
df2 <- subset(df, select = c(Daily.Time.Spent.on.Site,Age,Area.Income,Daily.Internet.Usage,Ad.Topic.Line,City,Male,Country,Clicked.on.Ad))
names(df2)
```

```{r}
set.seed(1234)
# Randomizing the rows, creates a uniform distribution of 150
random <- runif(150)
df_random <- df2[order(random),]
# Selecting the first 6 rows from iris_random
head(df_random)
```


```{r}
normal <- function(x) (
  return( ((x - min(x)) /(max(x)-min(x))) )
)
normal(1:9)
df_new <- as.data.frame(lapply(df_random[,-9], normal))
summary(df_new)
```
```{r}
# Lets now create test and train data sets
train <- df_new[1:130,]
test <- df_new[131:150,]
train_sp <- df_random[1:130,9]
test_sp <- df_random[131:150,9]
```

```{r}
# Lets build a model on it; cl is the class of the training data set and k is the no of neighbours to look for 
# in order to classify it accordingly
library(class)  
require(class)
model <- knn(train= train,test=test, ,cl= train_sp,k=13)
table(factor(model))
fd<-table(test_sp,model)
fd
```

```{r}
accuracy <- function(x){sum(diag(x)/(sum(rowSums(x)))) * 100}
accuracy(fd)
```
#### **SVM**
```{r}
library(caret)
```

```{r}
# So, 70% of the data is used for training and the remaining 30% is #for testing the model.
# - The “list” parameter is for whether to return a list or matrix. 
# We are passing FALSE for not returning a list
# 
intrain <- createDataPartition(y = df2$Clicked.on.Ad, p= 0.7, list = FALSE)
training <- df2[intrain,]
testing <- df2[-intrain,]
```

```{r}
# We check the dimensions of out training dataframe and testing #dataframe
dim(training); 
dim(testing);
```
```{r}
#Changing our target variable to factor
training[["Clicked.on.Ad"]] = factor(training[["Clicked.on.Ad"]])
```

```{r}
# The trainControl method will take three parameters:
# a) The “method” parameter defines the resampling method, 
# in this demo we’ll be using the repeatedcv or the repeated cross-validation method.
# b) The next parameter is the “number”, this basically holds the number of resampling iterations.
# c) The “repeats ” parameter contains the sets to compute for our repeated cross-validation. 
# We are using setting number =10 and repeats =3
trctrl <- trainControl(method = "repeatedcv", number = 10, repeats = 3)
svm_Linear <- train(Clicked.on.Ad ~., data = training, method = "svmLinear",
trControl=trctrl,
preProcess = c("center", "scale"),
tuneLength = 10)
```

```{r}
svm_Linear
```
```{r}
# We can use the predict() method for predicting results as shown below. 
# We pass 2 arguements, our trained model and our testing data frame.
# ---
# 
test_pred <- predict(svm_Linear, newdata = testing)
test_pred
```


```{r}
# Now checking for our accuracy of our model by using a confusion matrix 
# ---
# 
confusionMatrix(table(test_pred, testing$Clicked.on.Ad))
```
#### **Naive Bayes**
```{r}
#install.packages("caretEnsemble",,dependencies = TRUE, repos = 'http://cran.rstudio.com/')
#install.packages("Amelia",dependencies = TRUE, repos = 'http://cran.rstudio.com/')
#install.packages("mice",dependencies = TRUE, repos = 'http://cran.rstudio.com/')
#install.packages("GGally",dependencies = TRUE, repos = 'http://cran.rstudio.com/')
```



```{r}
library(tidyverse)
library(ggplot2)
library(caret)#confusionMatrix
library(caretEnsemble)
library(psych)
library(Amelia)#missmap
library(mice) #mice
library(GGally) #ggpairs
library(rpart)
library(randomForest)
library(tidyverse)
```
```{r}
# describing the data
summary(df2)
```
```{r}
# We convert the output variable into a categorical variable
df2$Clicked.on.Ad <- factor(df2$Clicked.on.Ad)
df2$Clicked.on.Ad
```





```{r}
# Splitting data into training and test data sets
# ---
# 
indxTrain <- caret::createDataPartition(y = df2$Clicked.on.Ad,p = 0.75,list = FALSE)
training <- df2[indxTrain,]
training
testing1 <- df2[-indxTrain,]
```
```{r}
# Checking dimensions of the split
prop.table(table(df2$Clicked.on.Ad)) * 100
prop.table(table(df2$Clicked.on.Ad)) * 100
prop.table(table(df2$Clicked.on.Ad)) * 100
```
```{r}
# Comparing the clicked on add of the training and testing phase
# Creating objects x which holds the predictor variables and y which holds the response variables
# ---
#
x = training[,-9]
colSums(is.na(x))
y = training$Clicked.on.Ad
```
```{r}
# Loading our inbuilt e1071 package that holds the Naive Bayes function.
library(e1071)
```

```{r}
# Now building our model 
model = train(x,y,'nb',trControl=trainControl(method='cv',number=10))
```
```{r}
# Model Evalution
# Predicting our testing set
# 
Predict <- predict(model,newdata = testing1 )
```
```{r}
# Getting the confusion matrix to see accuracy value and other parameter values
confusionMatrix(Predict, testing1$Clicked.on.Ad)
```
####  **Decision trees**
```{r}
set.seed(100)
id<- sample(2,nrow(df2),prob =c(0.7,0.3),replace = T)
traindf<- df2[id==1,]
testdf<- df2[id==2,]
```

```{r}
library(rpart)
library(rpart.plot)
m <- rpart(Clicked.on.Ad ~., data = df2,
           method = "class")
rpart.plot(m)
```

```{r}
p <- predict(m,df2, type = "class") 
cm <- table(p,df$Clicked.on.Ad)
cm
```

```{r}
accuracy <- function(x){sum(diag(x)/(sum(rowSums(x)))) * 100}
accuracy(cm)
```
#### **HYPERPARAMETER TUNING FOR DECISION TREES**
Training the decision tree model
```{r}
df$Timestamp<- NULL
names(df)
```

```{r}
names(df)
make.names(names(df))
colnames(df) <- make.names(colnames(df),unique = T)
```
```{r}
#install.packages("mlr",,dependencies = TRUE, repos = 'http://cran.rstudio.com/')
library(mlr)
dfTask <- makeClassifTask(data = df, target = "Clicked.on.Ad")
tree <- makeLearner("classif.rpart")
```

```{r}
# Printing available rpart hyperparameters
ls()
getParamSet(tree)
```

##### Defining the hyperparameter space for tuning
```{r}
treeParamSpace <- makeParamSet(
  makeIntegerParam("minsplit", lower = 5, upper = 20),
  makeIntegerParam("minbucket", lower = 3, upper = 10),
  makeNumericParam("cp", lower = 0.01, upper = 0.1),
  makeIntegerParam("maxdepth", lower = 3, upper = 10))
```

```{r}
# Defining the random search
randSearch <- makeTuneControlRandom(maxit = 200)
cvForTuning <- makeResampleDesc("CV", iters = 5)
```

#### Performing hyperparameter tuning
```{r}
#install.packages("detector",dependencies = TRUE,repos = 'http://cran.rstudio.com/')
library(parallelMap)
library(detector)
library(parallel)
parallelStartSocket(cpus = detectCores())
tunedTreePars <- tuneParams(tree, task = dfTask,
                            resampling = cvForTuning,
                            par.set = treeParamSpace,
                            control = randSearch)
parallelStop()
```
##### Training the model with the tuned hyperparameters
```{r}
# Training the final tuned model
tunedTree <- setHyperPars(tree, par.vals = tunedTreePars$x)
tunedTreeModel <- train(tunedTree, dfTask)
```

##### Plotting the decision tree
```{r}
treeModelData <- getLearnerModel(tunedTreeModel)
rpart.plot(treeModelData, roundint = FALSE,
box.palette = "BuBn",
type = 5)
```

##### **Exploring the model**
```{r}
printcp(treeModelData, digits = 3)
```
```{r}
# Cross-validating the model-building process
outer <- makeResampleDesc("CV", iters = 5)
treeWrapper <- makeTuneWrapper("classif.rpart", resampling = cvForTuning,
                               par.set = treeParamSpace,
                               control = randSearch)
parallelStartSocket(cpus = detectCores())
cvWithTuning <- resample(treeWrapper,dfTask, resampling = outer)
parallelStop()
```

```{r}
# Extracting the cross-validation result
cvWithTuning
```
####  **Random Forest**
```{r}
library(randomForest)
dfforest <- randomForest(Clicked.on.Ad ~ Daily.Time.Spent.on.Site+Age+
             Area.Income+Daily.Internet.Usage+Ad.Topic.Line+
             City+Country+Male,data=traindf)
dfforest
```
```{r}
predforest <- predict(dfforest,testdf,type="class")
predforest
```
```{r}
confusionMatrix(table(predforest,testdf$Clicked.on.Ad))
```

```{r}
ranTree=table(predforest,testdf$Clicked.on.Ad)
ranTree
```
```{r}
accuracy <- function(x){sum(diag(x)/(sum(rowSums(x)))) * 100}
accuracy(ranTree)
```

```{r}
importance(dfforest)
```

```{r}
important <- importance(dfforest)
important
Important_Features <- data.frame(Feature = row.names(important), Importance = important[, 1])
Important_Features
```

```{r}
plot_ <- ggplot(Important_Features, 
    aes(x= reorder(Feature,
Importance) , y = Importance) ) +
geom_bar(stat = "identity", 
        fill = "#800080") +
coord_flip() +
theme_light(base_size = 20) +
xlab("") + 
ylab("Importance")+
ggtitle("Important Features in Random Forest\n") +
theme(plot.title = element_text(size=18))
ggsave("important_features.png", 
      plot_)
plot_
```







